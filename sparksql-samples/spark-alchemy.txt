https://github.com/swoop-inc/spark-alchemy/wiki/Spark-HyperLogLog-Functions

# DataProc 2.1
spark-shell --packages com.swoop:spark-alchemy_2.12:1.2.1
#  https://repo1.maven.org/maven2/com/swoop/spark-alchemy_2.12/1.2.1/spark-alchemy_2.12-1.2.1.jar
#  https://repo1.maven.org/maven2/net/agkn/hll/1.6.0/hll-1.6.0.jar
#  https://repo1.maven.org/maven2/it/unimi/dsi/fastutil/6.5.11/fastutil-6.5.11.jar

# DataProc 2.0
spark-shell --packages com.swoop:spark-alchemy_2.12:1.0.1
#  https://repo1.maven.org/maven2/com/swoop/spark-alchemy_2.12/1.0.1/spark-alchemy_2.12-1.0.1.jar
#  https://repo1.maven.org/maven2/net/agkn/hll/1.6.0/hll-1.6.0.jar
#  https://repo1.maven.org/maven2/it/unimi/dsi/fastutil/6.5.11/fastutil-6.5.11.jar


import com.swoop.alchemy.spark.expressions.hll.functions._
import org.apache.spark.sql.functions._

spark.range(100000).select(
  // exact distinct count
  countDistinct('id).as("cntd"),
  // Spark's HLL implementation with default 5% precision
  approx_count_distinct('id).as("anctd_spark_default"),
  // approximate distinct count with default 5% precision
  hll_cardinality(hll_init_agg('id)).as("acntd_default"),
  // approximate distinct counts with custom precision
  map(
    Seq(0.005, 0.02, 0.05, 0.1).flatMap { error =>
      lit(error) :: hll_cardinality(hll_init_agg('id, error)) :: Nil
    }: _*
  ).as("acntd")
).show(false)

com.swoop.alchemy.spark.expressions.hll.HLLFunctionRegistration.registerFunctions(spark)

spark.range(100000).createOrReplaceTempView("ids")

spark.sql("""
select
    -- exact distinct count
    count(distinct id) as cntd,
    -- Spark's HLL implementation with default 5% precision
    approx_count_distinct(id) as anctd_spark_default,
    -- approximate distinct count with default 5% precision
    hll_cardinality(hll_init_agg(id)) as acntd_default,
    -- approximate distinct counts with custom precision
    map(
        0.005, hll_cardinality(hll_init_agg(id, 0.005)),
        0.020, hll_cardinality(hll_init_agg(id, 0.020)),
        0.050, hll_cardinality(hll_init_agg(id, 0.050)),
        0.100, hll_cardinality(hll_init_agg(id, 0.100))
    ) as acntd
from ids""").show(false)

*****************

pyspark --packages com.swoop:spark-alchemy_2.12:1.2.1

from pyspark.sql import SparkSession
from pyspark.sql import Column
from pyspark import SparkContext
from pyspark.sql.functions import expr, col, array

sc._jvm.com.swoop.alchemy.spark.expressions.hll.HLLFunctionRegistration.registerFunctions(spark._jsparkSession)

ids = spark \
    .range(100000). \
    toDF('primary_id') \
    .withColumn('secondary_ids', array(col('primary_id') + 1, col('primary_id') + 2, col('primary_id') + 3))

def unsafe_sql(c: Column) -> str:
    return c._jc.expr.__call__().sql.__call__()

def hll_cardinality(c: Column) -> Column:
    """
    Returns the estimated cardinality of an HLL "sketch"
    """
    return expr('hll_cardinality({})'.format(unsafe_sql(c)))

def hll_init_agg(c: Column) -> Column:
    """
    Combines all input into a single HLL sketch
    """
    return expr('hll_init_agg({})'.format(unsafe_sql(c)))

ids.select(hll_cardinality(hll_init_agg(col('primary_id'))).alias('primary_ids_count')).show()
